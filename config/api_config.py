"""
API é…ç½®å’Œæ¨¡å‹ç®¡ç†ä¸­å¿ƒ
æ‰€æœ‰ API Key å’Œæ¨¡å‹è°ƒç”¨éƒ½é›†ä¸­åœ¨è¿™é‡Œç®¡ç†

é…ç½®è¯´æ˜ï¼š
1. æ•æ„Ÿä¿¡æ¯ï¼ˆAPI Key ç­‰ï¼‰å­˜å‚¨åœ¨ .env æ–‡ä»¶ä¸­
2. å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å†™æ‚¨çš„é…ç½®
3. .env æ–‡ä»¶ä¸ä¼šè¢« git è·Ÿè¸ªï¼Œå¯ä»¥å®‰å…¨ä¸Šä¼ åˆ° GitHub
"""

import os
from typing import Optional
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä» .env æ–‡ä»¶ï¼‰
try:
    from dotenv import load_dotenv
    
    # æŸ¥æ‰¾ .env æ–‡ä»¶ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
    env_path = Path(__file__).parent.parent / '.env'
    
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ“ å·²åŠ è½½é…ç½®æ–‡ä»¶: {env_path}")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
        print(f"â„¹ï¸  å°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼")
        print(f"â„¹ï¸  è¯·å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å†™é…ç½®")
        
except ImportError:
    print("âš ï¸  æœªå®‰è£… python-dotenv åŒ…")
    print("â„¹ï¸  è¯·è¿è¡Œ: pip install python-dotenv")
    print("â„¹ï¸  å°†ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼")

# ============================================
# ğŸ”‘ API KEY é…ç½®åŒºï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# ============================================

# ä»ç¯å¢ƒå˜é‡è¯»å– API Key
API_KEY = os.getenv("API_KEY", "")

# ============================================
# ğŸŒ API æ¥æºç«™ç‚¹é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# ============================================

# API æä¾›å•†ç±»å‹
API_PROVIDER = os.getenv("API_PROVIDER", "openai")

# API Base URL
API_BASE_URL = os.getenv("API_BASE_URL", "")

# ============================================
# ğŸ“ æ¨¡å‹é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# ============================================

# æ¨¡å‹åç§°
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-2.5-pro-free")

# æ¸©åº¦å‚æ•°
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.1"))

# Embedding æ¨¡å‹åç§°
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "BAAI/bge-base-zh-v1.5")

# Embedding è®¾å¤‡
EMBEDDING_DEVICE = os.getenv("EMBEDDING_DEVICE", "cuda")

# Embedding æ‰¹å¤„ç†å¤§å°ï¼ˆæ›´å¤§çš„å€¼ = æ›´å¿«ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
# ä½¿ç”¨åŠ¨æ€ç­–ç•¥ï¼šä»æ­¤å€¼å¼€å§‹ï¼Œå¤±è´¥åˆ™å‡åŠé‡è¯•
# RTX 5090 (32GB) å»ºè®®: 1024
# RTX 4090 (24GB) å»ºè®®: 512
# RTX 3090 (24GB) å»ºè®®: 256
EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "1024"))

# å¤š GPU æ”¯æŒï¼ˆå¦‚æœæœ‰å¤šå— GPUï¼‰
# è®¾ç½®ä¸º True å°†ä½¿ç”¨ DataParallel åœ¨å¤š GPU ä¸Šå¹¶è¡Œå¤„ç†
EMBEDDING_MULTI_GPU = os.getenv("EMBEDDING_MULTI_GPU", "false").lower() == "true"


# ============================================
# ğŸ”§ è¾…åŠ©å‡½æ•°
# ============================================

def get_google_api_key() -> str:
    """
    è·å– Google API Keyï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬æ–‡ä»¶é…ç½®ï¼‰
    
    Returns:
        str: API Key
        
    Raises:
        ValueError: å¦‚æœæœªé…ç½® API Key
    """
    # ä¼˜å…ˆä½¿ç”¨æœ¬æ–‡ä»¶é…ç½®çš„ API Key
    if API_KEY and API_KEY.strip():
        return API_KEY.strip()
    
    # å¤‡ç”¨ï¼šå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
    env_key = os.getenv("API_KEY")
    if env_key and env_key.strip():
        return env_key.strip()
    
    # å¦‚æœéƒ½æ²¡æœ‰ï¼ŒæŠ›å‡ºé”™è¯¯
    raise ValueError(
        "æœªé…ç½® API Keyï¼\n"
        "è¯·åœ¨ api_config.py æ–‡ä»¶ä¸­è®¾ç½® API_KEY = 'your-key'\n"
        "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export API_KEY='your-key'"
    )


def get_api_base_url() -> Optional[str]:
    """
    è·å– API Base URLï¼ˆå¦‚æœé…ç½®äº†è‡ªå®šä¹‰ç«¯ç‚¹ï¼‰
    
    Returns:
        Optional[str]: API Base URLï¼Œå¦‚æœæœªé…ç½®åˆ™è¿”å› Noneï¼ˆä½¿ç”¨é»˜è®¤ï¼‰
    """
    if API_BASE_URL and API_BASE_URL.strip():
        return API_BASE_URL.strip().rstrip('/')
    
    return None


def get_embedding_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æˆ–è¿”å›é…ç½®çš„ Embedding è®¾å¤‡
    
    Returns:
        str: "cuda", "mps" æˆ– "cpu"
    """
    if EMBEDDING_DEVICE.lower() != "auto":
        return EMBEDDING_DEVICE.lower()
    
    # è‡ªåŠ¨æ£€æµ‹
    try:
        import torch
        if torch.cuda.is_available():
            print("âœ“ æ£€æµ‹åˆ° CUDA GPUï¼ŒEmbedding å°†ä½¿ç”¨ GPU åŠ é€Ÿ")
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("âœ“ æ£€æµ‹åˆ° Apple MPSï¼ŒEmbedding å°†ä½¿ç”¨ GPU åŠ é€Ÿ")
            return "mps"
    except ImportError:
        pass
    
    print("â„¹ æœªæ£€æµ‹åˆ° GPUï¼ŒEmbedding å°†ä½¿ç”¨ CPU")
    return "cpu"


# ============================================
#  æ¨¡å‹å®ä¾‹åŒ–å‡½æ•°ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
# ============================================

def create_gemini_llm():
    """
    åˆ›å»º Gemini LLM å®ä¾‹ï¼ˆç»Ÿä¸€çš„æ¨¡å‹åˆ›å»ºå…¥å£ï¼‰
    æ ¹æ® API_PROVIDER é€‰æ‹©ä½¿ç”¨ Google å®˜æ–¹ API æˆ– OpenAI å…¼å®¹ API
    
    Returns:
        ChatGoogleGenerativeAI æˆ– ChatOpenAI: é…ç½®å¥½çš„ LLM å®ä¾‹
    """
    # è·å– API Key
    api_key = get_google_api_key()
    
    # è·å– API Base URLï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    base_url = get_api_base_url()
    
    print(f" æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {MODEL_NAME}")
    print(f"   Temperature: {TEMPERATURE}")
    print(f"   API æä¾›å•†: {API_PROVIDER}")
    if base_url:
        print(f"   API Base URL: {base_url}")
    else:
        print(f"   API Base URL: å®˜æ–¹ Google API")
    
    # æ ¹æ®æä¾›å•†ç±»å‹é€‰æ‹©ä¸åŒçš„å®ç°
    if API_PROVIDER.lower() == "openai":
        # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°ç¬¬ä¸‰æ–¹ Gemini ä»£ç†ï¼‰
        try:
            from langchain_openai import ChatOpenAI
        except ImportError:
            raise ImportError(
                "æœªå®‰è£… langchain-openai åŒ…\n"
                "è¯·è¿è¡Œ: pip install langchain-openai"
            )
        
        if not base_url:
            raise ValueError("ä½¿ç”¨ OpenAI æ¨¡å¼æ—¶ï¼Œå¿…é¡»é…ç½® API_BASE_URL")
        
        llm = ChatOpenAI(
            model=MODEL_NAME,
            temperature=TEMPERATURE,
            api_key=api_key,
            base_url=base_url
        )
        
    else:  # API_PROVIDER == "google"
        # ä½¿ç”¨ Google å®˜æ–¹ API (gRPC)
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
        except ImportError:
            raise ImportError(
                "æœªå®‰è£… langchain-google-genai åŒ…\n"
                "è¯·è¿è¡Œ: pip install langchain-google-genai"
            )
        
        llm_kwargs = {
            "model": MODEL_NAME,
            "temperature": TEMPERATURE,
            "google_api_key": api_key,
            "convert_system_message_to_human": True  # Gemini ä¸æ”¯æŒ system message
        }
        
        # æ³¨æ„ï¼šChatGoogleGenerativeAI ä¸æ”¯æŒè‡ªå®šä¹‰ base_urlï¼ˆgRPC åè®®ï¼‰
        if base_url:
            print("âš ï¸  è­¦å‘Š: Google å®˜æ–¹ API æ¨¡å¼ä¸æ”¯æŒè‡ªå®šä¹‰ base_url")
        
        llm = ChatGoogleGenerativeAI(**llm_kwargs)
    
    # ç®€å•æµ‹è¯•è¿æ¥ï¼ˆè·³è¿‡æµ‹è¯•ï¼Œå› ä¸ºæŸäº› API è¿”å›æ ¼å¼å¯èƒ½ä¸æ ‡å‡†ï¼‰
    print(f"âœ“ Gemini LLM å®ä¾‹åˆ›å»ºæˆåŠŸ")
    print(f"â„¹ï¸  é¦–æ¬¡è°ƒç”¨æ—¶å°†éªŒè¯ API è¿æ¥")
    
    return llm


def create_embedding_model():
    """
    åˆ›å»º Embedding æ¨¡å‹å®ä¾‹ï¼ˆç»Ÿä¸€çš„æ¨¡å‹åˆ›å»ºå…¥å£ï¼‰
    
    æ”¯æŒæ‰¹å¤„ç†å¤§å°é…ç½®ä»¥åŠ é€Ÿå¤„ç†ï¼š
    - æ›´å¤§çš„ batch_size = æ›´å¿«çš„å¤„ç†é€Ÿåº¦
    - éœ€è¦æ›´å¤š GPU æ˜¾å­˜
    
    å¤š GPU æ”¯æŒï¼š
    - è®¾ç½® EMBEDDING_MULTI_GPU=true å¯ç”¨
    - è‡ªåŠ¨æ£€æµ‹å¯ç”¨ GPU æ•°é‡å¹¶ä½¿ç”¨ DataParallel
    
    Returns:
        HuggingFaceEmbeddings: é…ç½®å¥½çš„ Embedding å®ä¾‹
    """
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
    except ImportError:
        raise ImportError(
            "æœªå®‰è£… langchain-huggingface åŒ…\n"
            "è¯·è¿è¡Œ: pip install langchain-huggingface sentence-transformers"
        )
    
    device = get_embedding_device()
    
    # æ£€æµ‹ GPU æ•°é‡
    gpu_count = 0
    if device == "cuda":
        try:
            import torch
            gpu_count = torch.cuda.device_count()
            print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ° {gpu_count} ä¸ª GPU")
        except ImportError:
            pass
    
    print(f"ğŸ”¤ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹: {EMBEDDING_MODEL_NAME}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {EMBEDDING_BATCH_SIZE}")
    print(f"   å¤š GPU æ¨¡å¼: {'å¯ç”¨' if EMBEDDING_MULTI_GPU and gpu_count > 1 else 'ç¦ç”¨'}")
    
    # é…ç½® encode_kwargs ä»¥å¯ç”¨æ‰¹å¤„ç†å’Œå…¶ä»–ä¼˜åŒ–
    # æ³¨æ„: ä¸è¦è®¾ç½® show_progress_barï¼Œlangchain_huggingface å†…éƒ¨ä¼šå¤„ç†
    encode_kwargs = {
        'batch_size': EMBEDDING_BATCH_SIZE,
        'normalize_embeddings': True,  # BGE æ¨¡å‹æ¨èå¯ç”¨å½’ä¸€åŒ–
    }
    
    # å¤š GPU é…ç½®
    model_kwargs = {'device': device}
    if EMBEDDING_MULTI_GPU and gpu_count > 1:
        # sentence-transformers æ”¯æŒå¤š GPU
        # é€šè¿‡è®¾ç½® device ä¸º None å¹¶åœ¨ encode æ—¶ä½¿ç”¨ multi_process_pool
        print(f"   âš¡ å¯ç”¨å¤š GPU å¹¶è¡Œå¤„ç† ({gpu_count} GPUs)")
        # å¯¹äºå¤š GPUï¼Œå¢å¤§æ‰¹å¤„ç†å¤§å°
        encode_kwargs['batch_size'] = EMBEDDING_BATCH_SIZE * gpu_count
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    
    print(f"âœ“ Embedding æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    return embeddings


# ============================================
# ğŸ“Š é…ç½®éªŒè¯
# ============================================

def validate_config():
    """
    éªŒè¯é…ç½®æ˜¯å¦å®Œæ•´
    
    Returns:
        bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
    """
    print("\n" + "="*60)
    print("  API é…ç½®éªŒè¯")
    print("="*60)
    
    all_ok = True
    
    # æ£€æŸ¥ API Key
    try:
        api_key = get_google_api_key()
        masked_key = api_key[:8] + "..." + api_key[-4:] if len(api_key) > 12 else "***"
        print(f"âœ“ Google API Key: {masked_key}")
    except ValueError as e:
        print(f"âœ— Google API Key: æœªé…ç½®")
        print(f"  {e}")
        all_ok = False
    
    # æ£€æŸ¥ API æä¾›å•†å’Œ Base URL
    print(f"âœ“ API æä¾›å•†: {API_PROVIDER}")
    base_url = get_api_base_url()
    if base_url:
        print(f"âœ“ API Base URL: {base_url}")
    else:
        if API_PROVIDER.lower() == "openai":
            print(f"âœ— API Base URL: æœªé…ç½®ï¼ˆOpenAI æ¨¡å¼éœ€è¦é…ç½®ï¼‰")
            all_ok = False
        else:
            print(f"âœ“ API Base URL: å®˜æ–¹ Google APIï¼ˆé»˜è®¤ï¼‰")
    
    # æ£€æŸ¥æ¨¡å‹é…ç½®
    print(f"âœ“ Gemini æ¨¡å‹: {MODEL_NAME}")
    print(f"âœ“ Embedding æ¨¡å‹: {EMBEDDING_MODEL_NAME}")
    print(f"âœ“ Embedding è®¾å¤‡: {EMBEDDING_DEVICE}")
    
    print("="*60)
    
    if all_ok:
        print("âœ“ æ‰€æœ‰é…ç½®éªŒè¯é€šè¿‡ï¼")
    else:
        print("âœ— é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")
    
    print()
    return all_ok


# ============================================
# ğŸ”§ ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
# ============================================

def get_llm():
    """å‘åå…¼å®¹çš„ LLM è·å–å‡½æ•°"""
    return create_gemini_llm()


def get_embeddings():
    """å‘åå…¼å®¹çš„ Embeddings è·å–å‡½æ•°"""
    return create_embedding_model()


# ============================================
# æµ‹è¯•ä»£ç 
# ============================================

if __name__ == "__main__":
    print("ğŸ§ª API é…ç½®æµ‹è¯•\n")
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("âš ï¸  è¯·å…ˆé…ç½® API Key åå†è¿è¡Œæµ‹è¯•")
        exit(1)
    
    print("\næµ‹è¯• 1: åˆ›å»º Gemini LLM")
    print("-" * 60)
    try:
        llm = create_gemini_llm()
        response = llm.invoke("ç”¨ä¸€å¥è¯ä»‹ç» DND 5E")
        print(f"æµ‹è¯•å›ç­”: {response.content[:100]}...")
        print("âœ“ LLM æµ‹è¯•é€šè¿‡\n")
    except Exception as e:
        print(f"âœ— LLM æµ‹è¯•å¤±è´¥: {e}\n")
    
    print("\næµ‹è¯• 2: åˆ›å»º Embedding æ¨¡å‹")
    print("-" * 60)
    try:
        embeddings = create_embedding_model()
        test_vec = embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
        print(f"å‘é‡ç»´åº¦: {len(test_vec)}")
        print("âœ“ Embedding æµ‹è¯•é€šè¿‡\n")
    except Exception as e:
        print(f"âœ— Embedding æµ‹è¯•å¤±è´¥: {e}\n")
    
    print("="*60)
    print("âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("="*60)
