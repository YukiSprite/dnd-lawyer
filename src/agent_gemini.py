"""
æ¨¡å—: Agent (src/agent_gemini.py)
é’ˆå¯¹ Gemini 1.5 Flash è¶…å¤§ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–çš„ç®€åŒ– Agent
"""

from typing import Dict, Any, List
import re
import numpy as np
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

from config import config

# Agent æç¤ºè¯æ¨¡æ¿
GEMINI_AGENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Pathfinder è§„åˆ™ä¸“å®¶åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œå‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## é‡è¦æŒ‡å¼•ï¼š
1. **å¼•ç”¨æ¥æº**ï¼šåœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºä¿¡æ¯æ¥æºï¼ˆä½¿ç”¨æ–‡æ¡£çš„ full_pathï¼‰
2. **ä¿æŒå‡†ç¡®**ï¼šä¸¥æ ¼åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. **ç»“æ„æ„ŸçŸ¥**ï¼šæ³¨æ„æ–‡æ¡£çš„å±‚çº§å…³ç³»ï¼ˆé€šè¿‡ full_path åˆ¤æ–­ï¼‰
4. **è¡¨æ ¼ç†è§£**ï¼šæ–‡æ¡£ä¸­å¯èƒ½åŒ…å« HTML è¡¨æ ¼ï¼Œè¯·æ­£ç¡®è§£æ
5. **å®Œæ•´å›ç­”**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¯·ç»¼åˆæ‰€æœ‰ç›¸å…³æ–‡æ¡£
6. **æœªæ‰¾åˆ°æ—¶**ï¼šå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥

## æ£€ç´¢åˆ°çš„è§„åˆ™æ–‡æ¡£ï¼š

{context}

## ç”¨æˆ·é—®é¢˜ï¼š

{input}

## ä½ çš„å›ç­”ï¼š
"""

GEMINI_PROMPT = ChatPromptTemplate.from_template(GEMINI_AGENT_TEMPLATE)


class GeminiAgentExecutor:

    def __init__(self, llm: ChatGoogleGenerativeAI, retriever: BaseRetriever, embedding_model=None):
        self.llm = llm
        self.retriever = retriever
        self.embedding_model = embedding_model  # ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤
        
        # èŠå¤©å†å²ï¼ˆä¿æŒæœ€è¿‘ K è½®å¯¹è¯ï¼‰
        self.chat_history: List[tuple] = []
        self.history_k = 5
        
        # åŠ¨æ€æ–‡æ¡£æ•°é‡æ§åˆ¶
        self.current_doc_count = config.PARENT_RETRIEVER_MAX_K  # ä»æœ€å¤§å€¼å¼€å§‹
        self.max_doc_count = config.PARENT_RETRIEVER_MAX_K
        self.min_doc_count = config.PARENT_RETRIEVER_MIN_K
        
        # åˆ›å»º chain
        self.chain = GEMINI_PROMPT | self.llm | StrOutputParser()

    def _calculate_semantic_similarity(self, query: str, doc: Document) -> float:
        """
        ä½¿ç”¨ embedding æ¨¡å‹è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        æ”¯æŒåŸºäºè·¯å¾„çš„ç›¸ä¼¼åº¦åŠ æƒï¼ˆæ­£å‘åŠ æƒå’Œè´Ÿå‘é™æƒï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            doc: æ–‡æ¡£
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)ï¼Œå¯èƒ½ç»è¿‡è·¯å¾„åŠ æƒè°ƒæ•´
            å¦‚æœæ–‡æ¡£è¢«æ’é™¤ï¼Œè¿”å› -1.0
        """
        if not self.embedding_model:
            return 1.0  # å¦‚æœæ²¡æœ‰ embedding æ¨¡å‹ï¼Œé»˜è®¤å…¨éƒ¨é€šè¿‡
        
        full_path = doc.metadata.get('full_path', '')
        source_title = doc.metadata.get('source_title', '')
        
        # ğŸ†• è·¯å¾„æ’é™¤ï¼šå¦‚æœå¯ç”¨ä¸”æ–‡æ¡£è·¯å¾„åŒ¹é…æ’é™¤è§„åˆ™ï¼Œç›´æ¥è¿”å› -1
        if getattr(config, 'ENABLE_PATH_EXCLUSION', False):
            exclusion_rules = getattr(config, 'PATH_EXCLUSION_RULES', [])
            for exclusion_keyword in exclusion_rules:
                if exclusion_keyword in full_path:
                    # print(f"[Agent] è·¯å¾„æ’é™¤: {full_path} åŒ¹é… '{exclusion_keyword}'ï¼Œè·³è¿‡")
                    return -1.0  # æ ‡è®°ä¸ºæ’é™¤
        
        try:
            # è·å–æŸ¥è¯¢çš„ embedding
            query_embedding = self.embedding_model.embed_query(query)
            
            # ğŸ”§ æ”¹è¿›ï¼šç»¼åˆè®¡ç®—æ ‡é¢˜ã€è·¯å¾„å’Œå†…å®¹çš„ç›¸ä¼¼åº¦
            # 1. è®¡ç®—ä¸æ ‡é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆæƒé‡æœ€é«˜ï¼Œå› ä¸ºæ ‡é¢˜æœ€èƒ½ä»£è¡¨æ–‡æ¡£ä¸»é¢˜ï¼‰
            title_similarity = 0.0
            if source_title:
                title_embedding = self.embedding_model.embed_query(source_title)
                title_vec = np.array(title_embedding)
                query_vec = np.array(query_embedding)
                title_similarity = float(np.dot(query_vec, title_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(title_vec)))
            
            # 2. è®¡ç®—ä¸è·¯å¾„æœ€åéƒ¨åˆ†çš„ç›¸ä¼¼åº¦ï¼ˆåŒ…å«å…·ä½“æ¡ç›®åç§°ï¼‰
            path_similarity = 0.0
            if full_path:
                # å–è·¯å¾„çš„æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯å…·ä½“æ¡ç›®åç§°ï¼‰
                path_last_part = full_path.split('/')[-1] if '/' in full_path else full_path
                path_embedding = self.embedding_model.embed_query(path_last_part)
                path_vec = np.array(path_embedding)
                query_vec = np.array(query_embedding)
                path_similarity = float(np.dot(query_vec, path_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(path_vec)))
            
            # 3. è®¡ç®—ä¸å†…å®¹æ‘˜è¦çš„ç›¸ä¼¼åº¦ï¼ˆå–å‰500å­—ç¬¦ï¼Œé¿å…å™ªéŸ³ï¼‰
            content_similarity = 0.0
            doc_text = doc.page_content[:500]
            if doc_text:
                doc_embedding = self.embedding_model.embed_query(doc_text)
                doc_vec = np.array(doc_embedding)
                query_vec = np.array(query_embedding)
                content_similarity = float(np.dot(query_vec, doc_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)))
            
            # 4. ç»¼åˆç›¸ä¼¼åº¦ï¼šæ ‡é¢˜ > è·¯å¾„ > å†…å®¹
            # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œä¼˜å…ˆè€ƒè™‘æ ‡é¢˜åŒ¹é…
            base_similarity = max(
                title_similarity * 1.5,      # æ ‡é¢˜å®Œå…¨åŒ¹é…æœ€é‡è¦
                path_similarity * 0.8,      # è·¯å¾„åŒ¹é…
                content_similarity * 0.8,    # å†…å®¹åŒ¹é…æƒé‡ç¨ä½
                (title_similarity * 0.5 + path_similarity * 0.3 + content_similarity * 0.2)  # åŠ æƒå¹³å‡
            )
            
            # è·¯å¾„åŠ æƒï¼šæ”¯æŒæ­£å‘åŠ æƒï¼ˆæå‡ï¼‰å’Œè´Ÿå‘åŠ æƒï¼ˆé™ä½ï¼‰
            if getattr(config, 'ENABLE_PATH_BOOSTING', False):
                boost_rules = getattr(config, 'PATH_BOOST_RULES', {})
                
                for path_keyword, boost_value in boost_rules.items():
                    if path_keyword in full_path:
                        # åº”ç”¨åŠ æƒï¼ˆæ­£å€¼æå‡ï¼Œè´Ÿå€¼é™ä½ï¼‰
                        boosted_similarity = base_similarity + boost_value
                        # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨ [0, 1] èŒƒå›´å†…
                        boosted_similarity = max(0.0, min(1.0, boosted_similarity))
                        
                        # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                        if boost_value >= 0:
                            print(f"[Agent] è·¯å¾„åŠ æƒâ†‘: {full_path[:50]}... åŒ¹é… '{path_keyword}', {base_similarity:.3f} â†’ {boosted_similarity:.3f} (+{boost_value})")
                            pass
                        else:
                            print(f"[Agent] è·¯å¾„é™æƒâ†“: {full_path[:50]}... åŒ¹é… '{path_keyword}', {base_similarity:.3f} â†’ {boosted_similarity:.3f} ({boost_value})")
                            pass
                        
                        return boosted_similarity
            
            return base_similarity
        except Exception as e:
            print(f"[Agent] è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 1.0  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡
    
    def _calculate_doc_to_doc_similarity(self, doc1: Document, doc2: Document) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        ç»¼åˆè€ƒè™‘æ ‡é¢˜å’Œå†…å®¹ï¼Œé¿å…ç»“æ„ç›¸ä¼¼ä½†ä¸»é¢˜ä¸åŒçš„æ–‡æ¡£è¢«è¯¯åˆ¤ä¸ºé‡å¤
        
        Args:
            doc1: æ–‡æ¡£1
            doc2: æ–‡æ¡£2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not self.embedding_model:
            return 0.0
        
        try:
            # è·å–ä¸¤ä¸ªæ–‡æ¡£çš„æ ‡é¢˜
            title1 = doc1.metadata.get('source_title', '')
            title2 = doc2.metadata.get('source_title', '')
            
            # ğŸ”§ æ”¹è¿›ï¼šé¦–å…ˆæ¯”è¾ƒæ ‡é¢˜ç›¸ä¼¼åº¦
            # å¦‚æœæ ‡é¢˜æ˜æ˜¾ä¸åŒï¼Œåˆ™è®¤ä¸ºä¸æ˜¯é‡å¤æ–‡æ¡£
            title_similarity = 0.0
            if title1 and title2:
                title1_embedding = self.embedding_model.embed_query(title1)
                title2_embedding = self.embedding_model.embed_query(title2)
                
                vec1 = np.array(title1_embedding)
                vec2 = np.array(title2_embedding)
                
                title_similarity = float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))
                
                # å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œç›´æ¥åˆ¤å®šä¸ºä¸é‡å¤
                # é¿å… "æ°”åŒ–å½¢ä½“" å’Œ "é»‘æš—è§†è§‰" è¿™ç§æ ‡é¢˜å®Œå…¨ä¸åŒçš„æ³•æœ¯è¢«è¯¯åˆ¤
                if title_similarity < 0.75:
                    return title_similarity  # è¿”å›è¾ƒä½çš„æ ‡é¢˜ç›¸ä¼¼åº¦
            
            # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆå–æ›´å¤šå†…å®¹ä»¥è·å–æ•ˆæœæè¿°ï¼‰
            doc1_text = doc1.page_content[:1500]
            doc2_text = doc2.page_content[:1500]
            
            doc1_embedding = self.embedding_model.embed_query(doc1_text)
            doc2_embedding = self.embedding_model.embed_query(doc2_text)
            
            vec1 = np.array(doc1_embedding)
            vec2 = np.array(doc2_embedding)
            
            content_similarity = float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))
            
            # ç»¼åˆç›¸ä¼¼åº¦ï¼šæ ‡é¢˜æƒé‡æ›´é«˜
            # åªæœ‰æ ‡é¢˜å’Œå†…å®¹éƒ½ç›¸ä¼¼æ—¶ï¼Œæ‰åˆ¤å®šä¸ºé‡å¤æ–‡æ¡£
            combined_similarity = min(
                title_similarity * 0.6 + content_similarity * 0.4,  # åŠ æƒå¹³å‡
                title_similarity + 0.15  # æ ‡é¢˜ä¸åŒæ—¶ï¼Œé™åˆ¶æœ€é«˜ç›¸ä¼¼åº¦
            )
            
            return combined_similarity
            
        except Exception as e:
            print(f"[Agent] è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _deduplicate_and_refill_documents(
        self, 
        query: str, 
        initial_docs: List[Document], 
        target_count: int = 30,
        similarity_threshold: float = 0.80,
        max_attempts: int = 3
    ) -> List[Document]:
        """
        å»é‡ç›¸ä¼¼æ–‡æ¡£å¹¶åŠ¨æ€è¡¥å……ï¼Œç¡®ä¿è¿”å›è¶³å¤Ÿæ•°é‡çš„ç‹¬ç‰¹æ–‡æ¡£
        
        ç­–ç•¥ï¼š
        1. å¯¹åˆå§‹æ–‡æ¡£æŒ‰ç›¸ä¼¼åº¦å»é‡
        2. å¦‚æœå»é‡åä¸è¶³ç›®æ ‡æ•°é‡ï¼Œå¢åŠ æ£€ç´¢æ•°é‡é‡æ–°æ£€ç´¢
        3. å¯¹æ–°æ£€ç´¢ç»“æœå»é‡ï¼ˆåŒ…æ‹¬ä¸å·²ä¿ç•™æ–‡æ¡£æ¯”è¾ƒï¼‰
        4. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡æˆ–è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            initial_docs: åˆå§‹æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            target_count: ç›®æ ‡æ–‡æ¡£æ•°é‡
            similarity_threshold: æ–‡æ¡£é—´ç›¸ä¼¼åº¦é˜ˆå€¼
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°
            
        Returns:
            å»é‡å¹¶è¡¥å……åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.embedding_model:
            print("[Agent] âš ï¸  æœªæä¾› embedding æ¨¡å‹ï¼Œè·³è¿‡æ–‡æ¡£å»é‡")
            return initial_docs[:target_count]
        
        if not initial_docs:
            return initial_docs
        
        print(f"[Agent] æ­£åœ¨è¿›è¡Œæ–‡æ¡£å»é‡ä¸åŠ¨æ€è¡¥å……ï¼ˆç›®æ ‡: {target_count} ä¸ªç‹¬ç‰¹æ–‡æ¡£ï¼Œé˜ˆå€¼: {similarity_threshold}ï¼‰...")
        
        unique_docs = []
        all_checked_docs = set()  # ä½¿ç”¨setå­˜å‚¨å·²æ£€æŸ¥æ–‡æ¡£çš„IDï¼Œé¿å…é‡å¤æ£€æŸ¥
        skipped_count = 0
        
        # ç¬¬ä¸€è½®ï¼šå¤„ç†åˆå§‹æ–‡æ¡£
        print(f"\n[Agent] ç¬¬ 1 è½®ï¼šå¤„ç†åˆå§‹ {len(initial_docs)} ä¸ªæ–‡æ¡£...")
        for i, current_doc in enumerate(initial_docs):
            # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†
            doc_id = f"{current_doc.metadata.get('full_path', 'unknown')}::{current_doc.page_content[:100]}"
            
            if doc_id in all_checked_docs:
                continue
            all_checked_docs.add(doc_id)
            
            is_duplicate = False
            current_title = current_doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
            
            # ä¸å·²ä¿ç•™çš„æ–‡æ¡£æ¯”è¾ƒ
            for kept_doc in unique_docs:
                doc_similarity = self._calculate_doc_to_doc_similarity(current_doc, kept_doc)
                
                if doc_similarity >= similarity_threshold:
                    # å‘ç°é‡å¤æ–‡æ¡£
                    kept_title = kept_doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
                    print(f"  âœ— è·³è¿‡: {current_title}... ä¸ {kept_title} (ç›¸ä¼¼åº¦={doc_similarity:.3f}, é‡å¤)")
                    is_duplicate = True
                    skipped_count += 1
                    break
            
            if not is_duplicate:
                unique_docs.append(current_doc)
                if i < 10 or len(unique_docs) <= 5:
                    print(f"  âœ“ ä¿ç•™: {current_title}...")
        
        print(f"[Agent] ç¬¬ 1 è½®å®Œæˆ: {len(initial_docs)} ä¸ªæ–‡æ¡£ â†’ {len(unique_docs)} ä¸ªç‹¬ç‰¹æ–‡æ¡£")
        
        # å¦‚æœå·²ç»è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
        if len(unique_docs) >= target_count:
            print(f"[Agent] âœ“ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ ({target_count})ï¼Œæ— éœ€è¡¥å……")
            return unique_docs[:target_count]
        
        # éœ€è¦è¡¥å……çš„è½®æ¬¡
        attempt = 1
        retrieve_multiplier = 2  # æ¯æ¬¡å¢åŠ æ£€ç´¢æ•°é‡çš„å€æ•°
        
        while len(unique_docs) < target_count and attempt < max_attempts:
            attempt += 1
            needed = target_count - len(unique_docs)
            
            # è®¡ç®—æ–°çš„æ£€ç´¢æ•°é‡ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰
            new_retrieve_count = len(initial_docs) * (retrieve_multiplier ** attempt)
            new_retrieve_count = min(new_retrieve_count, 200)  # æé«˜ä¸Šé™
            
            # print(f"\n[Agent] ç¬¬ {attempt} è½®ï¼šè¿˜éœ€ {needed} ä¸ªæ–‡æ¡£ï¼Œæ­£åœ¨æ£€ç´¢ {new_retrieve_count} ä¸ªå€™é€‰...")
            
            try:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç›´æ¥ä» ParentDocumentRetriever çš„ vectorstore æ£€ç´¢æ›´å¤šå­æ–‡æ¡£
                # ä½¿ç”¨ getattr é¿å…ç±»å‹æ£€æŸ¥é”™è¯¯
                if not hasattr(self.retriever, 'vectorstore') or not hasattr(self.retriever, 'docstore'):
                    print(f"[Agent] âš ï¸  æ£€ç´¢å™¨ä¸æ”¯æŒåŠ¨æ€æ•°é‡æ£€ç´¢ï¼Œåœæ­¢è¡¥å……")
                    break
                
                vectorstore = getattr(self.retriever, 'vectorstore')
                docstore = getattr(self.retriever, 'docstore')
                
                # ä»å‘é‡æ•°æ®åº“æ£€ç´¢æ›´å¤šå­æ–‡æ¡£
                child_docs = vectorstore.similarity_search(query, k=new_retrieve_count)
                
                # ä»å­æ–‡æ¡£ ID è·å–çˆ¶æ–‡æ¡£
                more_docs = []
                for child_doc in child_docs:
                    # ParentDocumentRetriever åœ¨å­æ–‡æ¡£çš„ metadata ä¸­å­˜å‚¨çˆ¶æ–‡æ¡£ ID
                    parent_doc_id = child_doc.metadata.get("doc_id")
                    if parent_doc_id and parent_doc_id in docstore.store:
                        parent_doc = docstore.store[parent_doc_id]
                        more_docs.append(parent_doc)
                
                print(f"[Agent] ä» {len(child_docs)} ä¸ªå­æ–‡æ¡£æ£€ç´¢åˆ° {len(more_docs)} ä¸ªçˆ¶æ–‡æ¡£")
                
                if not more_docs or len(more_docs) <= len(all_checked_docs):
                    print(f"[Agent] âš ï¸  æ²¡æœ‰è·å–åˆ°æ–°æ–‡æ¡£ï¼Œåœæ­¢è¡¥å……")
                    break
                
                # è®¡ç®—æ–°æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦å¹¶æ’åº
                new_candidates = []
                for doc in more_docs:
                    doc_id = f"{doc.metadata.get('full_path', 'unknown')}::{doc.page_content[:100]}"
                    
                    # è·³è¿‡å·²æ£€æŸ¥è¿‡çš„æ–‡æ¡£
                    if doc_id in all_checked_docs:
                        continue
                    
                    sim = self._calculate_semantic_similarity(query, doc)
                    new_candidates.append((doc, sim, doc_id))
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                new_candidates.sort(key=lambda x: x[1], reverse=True)
                
                # print(f"[Agent] è·å–åˆ° {len(new_candidates)} ä¸ªæ–°å€™é€‰æ–‡æ¡£")
                
                # å¤„ç†æ–°å€™é€‰æ–‡æ¡£
                added_in_round = 0
                for doc, sim, doc_id in new_candidates:
                    if len(unique_docs) >= target_count:
                        break
                    
                    all_checked_docs.add(doc_id)
                    
                    is_duplicate = False
                    current_title = doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
                    
                    # ä¸å·²ä¿ç•™çš„æ–‡æ¡£æ¯”è¾ƒ
                    for kept_doc in unique_docs:
                        doc_similarity = self._calculate_doc_to_doc_similarity(doc, kept_doc)
                        
                        if doc_similarity >= similarity_threshold:
                            kept_title = kept_doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
                            # print(f"  âœ— è·³è¿‡: {current_title}...ä¸{kept_title} (ç›¸ä¼¼åº¦={doc_similarity:.3f})")
                            is_duplicate = True
                            skipped_count += 1
                            break
                    
                    if not is_duplicate:
                        unique_docs.append(doc)
                        added_in_round += 1
                        if added_in_round <= 5:
                            # print(f"  âœ“ æ–°å¢: {current_title}... (ä¸æŸ¥è¯¢ç›¸ä¼¼åº¦={sim:.3f})")
                            pass
                
                print(f"[Agent] ç¬¬ {attempt} è½®å®Œæˆ: æ–°å¢ {added_in_round} ä¸ªç‹¬ç‰¹æ–‡æ¡£ï¼Œå½“å‰å…± {len(unique_docs)} ä¸ª")
                
                if added_in_round == 0:
                    print(f"[Agent] âš ï¸  æœ¬è½®æœªæ‰¾åˆ°æ–°çš„ç‹¬ç‰¹æ–‡æ¡£ï¼Œåœæ­¢è¡¥å……")
                    break
                    
            except Exception as e:
                print(f"[Agent] âš ï¸  ç¬¬ {attempt} è½®æ£€ç´¢æ—¶å‡ºé”™: {e}")
                break
        
        final_count = len(unique_docs)
        total_checked = len(all_checked_docs)
        
        # print(f"\n[Agent] âœ“ å»é‡ä¸è¡¥å……å®Œæˆ:")
        # print(f"    - æ£€æŸ¥äº† {total_checked} ä¸ªæ–‡æ¡£")
        # print(f"    - ä¿ç•™äº† {final_count} ä¸ªç‹¬ç‰¹æ–‡æ¡£")
        # print(f"    - ç§»é™¤äº† {skipped_count} ä¸ªé‡å¤æ–‡æ¡£")
        # print(f"    - è¾¾æˆç‡: {final_count}/{target_count} ({final_count/target_count*100:.1f}%)")
        
        if final_count < target_count:
            # print(f"[Agent] âš ï¸  æœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œå¯èƒ½éœ€è¦ï¼š")
            # print(f"    1. é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ (å½“å‰: {similarity_threshold})")
            # print(f"    2. å¢åŠ åˆå§‹æ£€ç´¢æ•°é‡ (å½“å‰: {len(initial_docs)})")
            pass
        
        return unique_docs
    
    def _filter_docs_by_similarity(self, query: str, docs: List[Document], threshold: float = 0.5, mode: str = "rank") -> List[Document]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤æˆ–æ’åºæ–‡æ¡£
        åŒæ—¶å¤„ç†è·¯å¾„æ’é™¤è§„åˆ™
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)ï¼Œä»…åœ¨ mode="threshold" æ—¶ä½¿ç”¨
            mode: "rank" (æŒ‰ç›¸ä¼¼åº¦æ’åº) æˆ– "threshold" (è¿‡æ»¤ä½äºé˜ˆå€¼çš„æ–‡æ¡£)
            
        Returns:
            è¿‡æ»¤/æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.embedding_model:
            print("[Agent] âš ï¸  æœªæä¾› embedding æ¨¡å‹ï¼Œè·³è¿‡è¯­ä¹‰è¿‡æ»¤")
            return docs
        
        if not docs:
            return docs
        
        if mode == "rank":
            print(f"[Agent] æ­£åœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶æ’åº...")
        else:
            print(f"[Agent] æ­£åœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆé˜ˆå€¼: {threshold}ï¼‰...")
        
        # è®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
        doc_scores = []
        excluded_count = 0
        
        for i, doc in enumerate(docs):
            similarity = self._calculate_semantic_similarity(query, doc)
            
            # ç›¸ä¼¼åº¦ä¸º -1 è¡¨ç¤ºè¢«æ’é™¤
            if similarity < 0:
                excluded_count += 1
                continue
            
            doc_scores.append({
                'doc': doc,
                'similarity': similarity,
                'index': i
            })
        
        if excluded_count > 0:
            print(f"[Agent] è·¯å¾„æ’é™¤: å·²è¿‡æ»¤ {excluded_count} ä¸ªä¸ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£")
        
        if mode == "rank":
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            doc_scores.sort(key=lambda x: x['similarity'], reverse=True)
            
            # æ˜¾ç¤ºæ’åºç»“æœï¼ˆå‰15ä¸ªï¼Œå› ä¸ºåé¢ä¼šå»é‡ï¼‰
            print(f"\n[Agent] æ–‡æ¡£ç›¸ä¼¼åº¦æ’åï¼ˆå‰ {min(15, len(doc_scores))} ä¸ªï¼‰ï¼š")
            for rank, item in enumerate(doc_scores[:15], 1):
                title = item['doc'].metadata.get('source_title', 'æœªçŸ¥')
                full_path = item['doc'].metadata.get('full_path', 'æœªçŸ¥')
                category = full_path.split('/')[0] if '/' in full_path else 'æœªçŸ¥'
                
                # æ ‡æ³¨ç‰ˆæœ¬ä¿¡æ¯
                version_tag = ""
                if "2024" in full_path or "2025" in full_path:
                    version_tag = " ğŸ†•"
                elif any(old in full_path for old in ["ç©å®¶æ‰‹å†Œ/", "åŸä¸»æŒ‡å—/", "æ€ªç‰©å›¾é‰´/"]):
                    version_tag = " ğŸ“œ"
                
                print(f"  {rank}. [{category}]{version_tag} {title[:35]}... ç›¸ä¼¼åº¦={item['similarity']:.3f}")
            
            # è¿”å›æ’åºåçš„æ–‡æ¡£
            filtered_docs = [item['doc'] for item in doc_scores]
            print(f"\n[Agent] è¯­ä¹‰æ’åºå®Œæˆ: {len(docs)} ä¸ªæ–‡æ¡£ â†’ {len(filtered_docs)} ä¸ªæ–‡æ¡£ï¼ˆå·²æ’é™¤ {excluded_count} ä¸ªï¼‰")
            
        else:  # mode == "threshold"
            # æŒ‰é˜ˆå€¼è¿‡æ»¤
            filtered_docs = []
            for item in doc_scores:
                title = item['doc'].metadata.get('source_title', 'æœªçŸ¥')
                print(f"  æ–‡æ¡£ {item['index']+1}: {title[:30]}... ç›¸ä¼¼åº¦={item['similarity']:.3f}", end="")
                
                if item['similarity'] >= threshold:
                    filtered_docs.append(item['doc'])
                    print(" âœ“ ä¿ç•™")
                else:
                    print(" âœ— è¿‡æ»¤")
            
            print(f"[Agent] è¯­ä¹‰è¿‡æ»¤: {len(docs)} ä¸ªæ–‡æ¡£ â†’ {len(filtered_docs)} ä¸ªæ–‡æ¡£ï¼ˆå·²æ’é™¤ {excluded_count} ä¸ªï¼‰")
        
        return filtered_docs
    
    def _format_documents(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not docs:
            return "æœªæ£€ç´¢åˆ°ç›¸å…³è§„åˆ™æ–‡æ¡£ã€‚"
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            full_path = doc.metadata.get('full_path', 'æœªçŸ¥æ¥æº')
            source_title = doc.metadata.get('source_title', 'æœªçŸ¥æ ‡é¢˜')
            content = doc.page_content
            
            formatted.append(f"""
--- æ–‡æ¡£ {i} ---
**æ¥æºè·¯å¾„**: {full_path}
**æ ‡é¢˜**: {source_title}

{content}
---
""")
        
        return "\n".join(formatted)

    def invoke(self, input_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›å“åº”ã€‚
        æ”¯æŒåŠ¨æ€è°ƒæ•´æ–‡æ¡£æ•°é‡ä»¥åº”å¯¹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚
        
        Args:
            input_dict: åŒ…å« 'input' é”®çš„å­—å…¸
            
        Returns:
            åŒ…å« 'output' é”®çš„å­—å…¸
        """
        user_input = input_dict.get("input", "æœªçŸ¥è¾“å…¥")
        
        try:
            print(f"\n[Agent] ç”¨æˆ·è¾“å…¥: {user_input}")
            
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆå°½å¯èƒ½å¤šï¼‰
            initial_retrieve_count = config.PARENT_RETRIEVER_TOP_K
            print(f"[Agent] æ­£åœ¨æ£€ç´¢æ–‡æ¡£ï¼ˆåˆå§‹æ£€ç´¢: {initial_retrieve_count} ä¸ªï¼‰...")
            retrieved_docs = self.retriever.invoke(user_input)
            print(f"[Agent] æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
            
            # 2. è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº/è¿‡æ»¤ï¼šä½¿ç”¨ embedding è‡ªåŠ¨åˆ¤æ–­æ–‡æ¡£ç›¸å…³æ€§ï¼ˆå¯é…ç½®ï¼‰
            if hasattr(config, 'ENABLE_SEMANTIC_FILTER') and config.ENABLE_SEMANTIC_FILTER:
                filter_mode = getattr(config, 'SEMANTIC_FILTER_MODE', 'rank')
                similarity_threshold = getattr(config, 'SEMANTIC_SIMILARITY_THRESHOLD', 0.4)
                
                retrieved_docs = self._filter_docs_by_similarity(
                    user_input, 
                    retrieved_docs, 
                    similarity_threshold,
                    mode=filter_mode
                )
            else:
                print(f"[Agent] è¯­ä¹‰è¿‡æ»¤å·²ç¦ç”¨ï¼Œè·³è¿‡è¿‡æ»¤")
            
            # 3. æ–‡æ¡£å»é‡å¹¶è¡¥å……ï¼šç§»é™¤å†…å®¹ç›¸ä¼¼çš„é‡å¤æ–‡æ¡£ï¼Œå¹¶åŠ¨æ€è¡¥å……ï¼ˆå¯é…ç½®ï¼‰
            if hasattr(config, 'ENABLE_DOCUMENT_DEDUPLICATION') and config.ENABLE_DOCUMENT_DEDUPLICATION:
                dedup_threshold = getattr(config, 'DOCUMENT_SIMILARITY_THRESHOLD', 0.80)
                target_doc_count = getattr(config, 'PARENT_RETRIEVER_TOP_K', 30)
                max_attempts = getattr(config, 'MAX_DEDUP_ATTEMPTS', 3)
                
                retrieved_docs = self._deduplicate_and_refill_documents(
                    user_input,
                    retrieved_docs,
                    target_count=target_doc_count,
                    similarity_threshold=dedup_threshold,
                    max_attempts=max_attempts
                )
            else:
                print(f"[Agent] æ–‡æ¡£å»é‡å·²ç¦ç”¨ï¼Œè·³è¿‡å»é‡")
            
            # 4. é™åˆ¶æ–‡æ¡£æ•°é‡åˆ°å½“å‰è®¾å®šå€¼ï¼ˆå–å‰Nä¸ªæœ€ç›¸å…³çš„ï¼‰
            final_doc_count = min(self.current_doc_count, len(retrieved_docs))
            retrieved_docs = retrieved_docs[:final_doc_count]
            print(f"[Agent] æœ€ç»ˆä½¿ç”¨ {len(retrieved_docs)} ä¸ªæ–‡æ¡£ï¼ˆæœ€ç›¸å…³çš„å‰ {final_doc_count} ä¸ªï¼‰")
            
            # 5. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self._format_documents(retrieved_docs)
            
            # 6. è°ƒç”¨ Geminiï¼ˆä¸ä¼ é€’å†å²ï¼Œåªä¼ é€’å½“å‰é—®é¢˜å’Œè§„åˆ™æ–‡æ¡£ï¼‰
            print("[Agent] æ­£åœ¨ç”Ÿæˆå›ç­”...")
            response = self.chain.invoke({
                "context": context,
                "input": user_input
            })
            
            # 7. æ‹¼æ¥æ–‡æ¡£æ¥æºè·¯å¾„åˆ°å›ç­”æœ«å°¾
            doc_sources = []
            for doc in retrieved_docs:
                full_path = doc.metadata.get('full_path', 'æœªçŸ¥æ¥æº')
                if full_path not in doc_sources:  # å»é‡
                    doc_sources.append(full_path)
            
            if doc_sources:
                sources_text = "\n\n" + "="*50 + "\n"
                sources_text += "ğŸ“š **å‚è€ƒçš„è§„åˆ™æ–‡æ¡£æ¥æº**ï¼š\n\n"
                for i, source in enumerate(doc_sources, 1):
                    sources_text += f"{i}. {source}\n"
                response_with_sources = response + sources_text
            else:
                response_with_sources = response
            
            # æˆåŠŸï¼å°è¯•å¢åŠ æ–‡æ¡£æ•°é‡ï¼ˆæ¸è¿›å¼å¢åŠ ï¼‰
            if self.current_doc_count < self.max_doc_count:
                self.current_doc_count = min(self.current_doc_count + 1, self.max_doc_count)
                print(f"[Agent] âœ“ å“åº”æˆåŠŸï¼Œä¸‹æ¬¡å°†å°è¯•ä½¿ç”¨ {self.current_doc_count} ä¸ªæ–‡æ¡£")
            
            # 8. ä¿å­˜å¯¹è¯å†å²ï¼ˆä»…ç”¨äºæœ¬åœ°è®°å½•ï¼Œä¸ä¼ é€’ç»™æ¨¡å‹ï¼‰
            self.chat_history.append((user_input, response_with_sources))
            if len(self.chat_history) > self.history_k:
                self.chat_history = self.chat_history[-self.history_k:]
            
            print("[Agent] å›ç­”ç”Ÿæˆå®Œæˆ")
            
            return {"output": response_with_sources}
            
        except Exception as e:
            error_str = str(e)
            
            # æ£€æµ‹æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™é”™è¯¯
            if "context length" in error_str.lower() or "max" in error_str.lower() and "token" in error_str.lower():
                print(f"[Agent] âš ï¸  ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™")
                
                # å‡å°‘æ–‡æ¡£æ•°é‡å¹¶é‡è¯•
                if self.current_doc_count > self.min_doc_count:
                    self.current_doc_count = max(self.current_doc_count - 4, self.min_doc_count)
                    print(f"[Agent] ğŸ“‰ å‡å°‘æ–‡æ¡£æ•°é‡åˆ° {self.current_doc_count}ï¼Œæ­£åœ¨é‡è¯•...")
                    
                    # é€’å½’é‡è¯•
                    return self.invoke(input_dict)
                else:
                    error_msg = f"æŠ±æ­‰ï¼Œå³ä½¿ä½¿ç”¨æœ€å°‘æ–‡æ¡£æ•°é‡ï¼ˆ{self.min_doc_count}ï¼‰ä»ç„¶è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶ã€‚è¯·å°è¯•æ›´ç®€çŸ­çš„é—®é¢˜ã€‚"
                    print(f"[Agent] âŒ {error_msg}")
                    self.chat_history.append((user_input, error_msg))
                    return {"output": error_msg}
            
            # å…¶ä»–é”™è¯¯
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {error_str}"
            print(f"[Agent] é”™è¯¯: {e}")
            
            # ä¿å­˜é”™è¯¯åˆ°å†å²
            self.chat_history.append((user_input, error_msg))
            
            return {"output": error_msg}


def create_gemini_agent_executor(
    llm: ChatGoogleGenerativeAI,
    retriever: BaseRetriever,
    embedding_model=None
) -> GeminiAgentExecutor:
    """
    åˆ›å»º Agent æ‰§è¡Œå™¨ã€‚
    
    Args:
        llm: Gemini LLM å®ä¾‹
        retriever: çˆ¶æ–‡æ¡£æ£€ç´¢å™¨å®ä¾‹
        embedding_model: Embedding æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰
        
    Returns:
        GeminiAgentExecutor å®ä¾‹
    """
    print("[Agent] æ­£åœ¨åˆ›å»º Agent æ‰§è¡Œå™¨...")
    if embedding_model:
        print("[Agent] âœ“ å·²å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ï¼ˆåŸºäº embedding æ¨¡å‹ï¼‰")
    return GeminiAgentExecutor(llm=llm, retriever=retriever, embedding_model=embedding_model)
