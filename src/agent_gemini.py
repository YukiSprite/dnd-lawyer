"""
æ¨¡å—: Agent (src/agent_gemini.py)
é’ˆå¯¹ Gemini 1.5 Flash è¶…å¤§ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–çš„ç®€åŒ– Agent
æ”¯æŒ PF (Pathfinder) å’Œ DND (Dungeons & Dragons) ä¸¤ç§è§„åˆ™ç‰ˆæœ¬

æ³¨æ„ï¼šè·¯å¾„åŠ æƒé€»è¾‘å·²ç§»è‡³ parent_retriever.py ä¸­çš„ PathBoostedRetriever
åœ¨æœç´¢é˜¶æ®µç›´æ¥åº”ç”¨åŠ æƒï¼Œè€Œä¸æ˜¯åå¤„ç†

æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå…³é”®è¯ä¼˜å…ˆ + è¯­ä¹‰è¡¥å……ï¼‰åœ¨æœ¬æ¨¡å—å†…å®ç°
"""

from typing import Dict, Any, List, Tuple, Set
from collections import defaultdict
import re
import math
import numpy as np
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

from config import config
from config.settings import get_current_version, get_version_info

# ============================================
# ç‰ˆæœ¬ç‰¹å®šçš„ Prompt æ¨¡æ¿
# ============================================

# Pathfinder è§„åˆ™ä¸“ç”¨ Prompt
PF_AGENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Pathfinder è§„åˆ™ä¸“å®¶åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œå‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## é‡è¦æŒ‡å¼•ï¼š
1. **å¼•ç”¨æ¥æº**ï¼šåœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºä¿¡æ¯æ¥æºï¼ˆä½¿ç”¨æ–‡æ¡£çš„ full_pathï¼‰
2. **ä¿æŒå‡†ç¡®**ï¼šä¸¥æ ¼åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. **ç»“æ„æ„ŸçŸ¥**ï¼šæ³¨æ„æ–‡æ¡£çš„å±‚çº§å…³ç³»ï¼ˆé€šè¿‡ full_path åˆ¤æ–­ï¼‰
4. **è¡¨æ ¼ç†è§£**ï¼šæ–‡æ¡£ä¸­å¯èƒ½åŒ…å« HTML è¡¨æ ¼ï¼Œè¯·æ­£ç¡®è§£æ
5. **å®Œæ•´å›ç­”**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¯·ç»¼åˆæ‰€æœ‰ç›¸å…³æ–‡æ¡£
6. **æœªæ‰¾åˆ°æ—¶**ï¼šå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥

## æ£€ç´¢åˆ°çš„è§„åˆ™æ–‡æ¡£ï¼š

{context}

## ç”¨æˆ·é—®é¢˜ï¼š

{input}

## ä½ çš„å›ç­”ï¼š
"""

# DND è§„åˆ™ä¸“ç”¨ Prompt
DND_AGENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ DND è§„åˆ™ä¸“å®¶åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œå‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

## é‡è¦æŒ‡å¼•ï¼š
1. **å¼•ç”¨æ¥æº**ï¼šåœ¨å›ç­”ä¸­æ˜ç¡®æŒ‡å‡ºä¿¡æ¯æ¥æºï¼ˆä½¿ç”¨æ–‡æ¡£çš„ full_pathï¼‰
2. **ä¿æŒå‡†ç¡®**ï¼šä¸¥æ ¼åŸºäºæä¾›çš„è§„åˆ™æ–‡æ¡£ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. **ç»“æ„æ„ŸçŸ¥**ï¼šæ³¨æ„æ–‡æ¡£çš„å±‚çº§å…³ç³»ï¼ˆé€šè¿‡ full_path åˆ¤æ–­ï¼‰
4. **è¡¨æ ¼ç†è§£**ï¼šæ–‡æ¡£ä¸­å¯èƒ½åŒ…å« HTML è¡¨æ ¼ï¼Œè¯·æ­£ç¡®è§£æ
5. **å®Œæ•´å›ç­”**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œè¯·ç»¼åˆæ‰€æœ‰ç›¸å…³æ–‡æ¡£
6. **æœªæ‰¾åˆ°æ—¶**ï¼šå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥

## æ£€ç´¢åˆ°çš„è§„åˆ™æ–‡æ¡£ï¼š

{context}

## ç”¨æˆ·é—®é¢˜ï¼š

{input}

## ä½ çš„å›ç­”ï¼š
"""

# æ ¹æ®ç‰ˆæœ¬é€‰æ‹© Prompt
def get_agent_template() -> str:
    """æ ¹æ®å½“å‰ç‰ˆæœ¬è·å–å¯¹åº”çš„ Prompt æ¨¡æ¿"""
    version = get_current_version()
    if version == "dnd":
        return DND_AGENT_TEMPLATE
    else:
        return PF_AGENT_TEMPLATE

def get_agent_prompt() -> ChatPromptTemplate:
    """è·å–å½“å‰ç‰ˆæœ¬çš„ ChatPromptTemplate"""
    return ChatPromptTemplate.from_template(get_agent_template())

# é»˜è®¤ä½¿ç”¨åŠ¨æ€è·å–çš„ Prompt
GEMINI_PROMPT = get_agent_prompt()


class GeminiAgentExecutor:

    def __init__(self, llm: ChatGoogleGenerativeAI, retriever: BaseRetriever, embedding_model=None):
        self.llm = llm
        self.retriever = retriever
        self.embedding_model = embedding_model  # ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤
        
        # èŠå¤©å†å²ï¼ˆä¿æŒæœ€è¿‘ K è½®å¯¹è¯ï¼‰
        self.chat_history: List[tuple] = []
        self.history_k = 5
        
        # åŠ¨æ€æ–‡æ¡£æ•°é‡æ§åˆ¶
        self.current_doc_count = config.PARENT_RETRIEVER_MAX_K  # ä»æœ€å¤§å€¼å¼€å§‹
        self.max_doc_count = config.PARENT_RETRIEVER_MAX_K
        self.min_doc_count = config.PARENT_RETRIEVER_MIN_K
        
        # åˆ›å»º chain
        self.chain = GEMINI_PROMPT | self.llm | StrOutputParser()
        
        # ============================================
        # å…³é”®è¯æ£€ç´¢ç´¢å¼•ï¼ˆå»¶è¿Ÿæ„å»ºï¼‰
        # ============================================
        self._keyword_index: Dict[str, List[Tuple[str, float]]] = {}  # {term: [(doc_id, score), ...]}
        self._doc_term_matrix: Dict[str, Dict[str, float]] = {}  # {doc_id: {term: score}}
        self._idf_scores: Dict[str, float] = {}  # {term: idf_score}
        self._doc_cache: Dict[str, Document] = {}  # {doc_id: Document}
        self._keyword_index_built: bool = False

    # ============================================
    # å…³é”®è¯æ£€ç´¢ç›¸å…³æ–¹æ³•
    # ============================================
    
    def _tokenize(self, text: str) -> List[str]:
        """
        ç®€å•åˆ†è¯ï¼šä¸­æ–‡ n-gram + è‹±æ–‡å•è¯
        
        Args:
            text: å¾…åˆ†è¯æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        tokens = []
        
        # æå–è‹±æ–‡å•è¯å’Œæ•°å­—ï¼ˆè½¬å°å†™ï¼‰
        english_pattern = r'[a-zA-Z0-9]+'
        english_tokens = re.findall(english_pattern, text.lower())
        tokens.extend(english_tokens)
        
        # æå–ä¸­æ–‡è¯ç»„
        chinese_pattern = r'[\u4e00-\u9fff]+'
        chinese_segments = re.findall(chinese_pattern, text)
        
        for segment in chinese_segments:
            # æ·»åŠ å®Œæ•´è¯ï¼ˆ2-10å­—ï¼‰
            if 2 <= len(segment) <= 10:
                tokens.append(segment)
            # æ·»åŠ  2-gram åˆ° 4-gram
            for n in range(2, min(5, len(segment) + 1)):
                for i in range(len(segment) - n + 1):
                    tokens.append(segment[i:i+n])
        
        return tokens
    
    def _build_keyword_index(self, docs: List[Document]):
        """
        ä¸ºæ–‡æ¡£åˆ—è¡¨æ„å»ºå…³é”®è¯ç´¢å¼•
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
        """
        if self._keyword_index_built and len(self._doc_cache) >= len(docs):
            return
        
        print(f"[Agent] æ­£åœ¨æ„å»ºå…³é”®è¯ç´¢å¼• ({len(docs)} ä¸ªæ–‡æ¡£)...")
        
        # æ¸…ç©ºæ—§ç´¢å¼•
        self._keyword_index.clear()
        self._doc_term_matrix.clear()
        self._idf_scores.clear()
        self._doc_cache.clear()
        
        # è®¡ç®—è¯é¢‘
        doc_term_freq: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
        doc_lengths: Dict[str, int] = {}
        term_doc_count: Dict[str, int] = defaultdict(int)
        
        for doc in docs:
            # ç”Ÿæˆæ–‡æ¡£ ID
            doc_id = f"{doc.metadata.get('full_path', 'unknown')}::{hash(doc.page_content[:200])}"
            self._doc_cache[doc_id] = doc
            
            # è·å–æ–‡æ¡£å†…å®¹
            content = doc.page_content
            title = doc.metadata.get('source_title', '')
            full_path = doc.metadata.get('full_path', '')
            
            # æ ‡é¢˜åŠ æƒï¼ˆå‡ºç°3æ¬¡ï¼‰
            full_text = f"{title} {title} {title} {full_path} {content}"
            tokens = self._tokenize(full_text)
            
            doc_lengths[doc_id] = len(tokens)
            seen_terms: Set[str] = set()
            
            for token in tokens:
                doc_term_freq[doc_id][token] += 1
                if token not in seen_terms:
                    term_doc_count[token] += 1
                    seen_terms.add(token)
        
        # è®¡ç®— IDF
        total_docs = len(docs)
        for term, doc_count in term_doc_count.items():
            self._idf_scores[term] = math.log((total_docs + 1) / (doc_count + 1)) + 1
        
        # è®¡ç®— TF-IDF å¹¶æ„å»ºå€’æ’ç´¢å¼•
        avg_doc_length = sum(doc_lengths.values()) / len(doc_lengths) if doc_lengths else 1
        
        for doc_id, term_freq in doc_term_freq.items():
            doc_len = doc_lengths[doc_id]
            self._doc_term_matrix[doc_id] = {}
            
            for term, freq in term_freq.items():
                # BM25 é£æ ¼çš„ TF å½’ä¸€åŒ–
                k1, b = 1.5, 0.75
                tf_norm = (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * doc_len / avg_doc_length))
                
                # TF-IDF åˆ†æ•°
                tf_idf = tf_norm * self._idf_scores[term]
                self._doc_term_matrix[doc_id][term] = tf_idf
                
                # æ›´æ–°å€’æ’ç´¢å¼•
                if term not in self._keyword_index:
                    self._keyword_index[term] = []
                self._keyword_index[term].append((doc_id, tf_idf))
        
        # å¯¹å€’æ’ç´¢å¼•ä¸­çš„æ–‡æ¡£æŒ‰åˆ†æ•°æ’åº
        for term in self._keyword_index:
            self._keyword_index[term].sort(key=lambda x: x[1], reverse=True)
        
        self._keyword_index_built = True
        print(f"[Agent] å…³é”®è¯ç´¢å¼•æ„å»ºå®Œæˆ: {len(self._keyword_index)} ä¸ªè¯é¡¹")
    
    def _keyword_search(self, query: str, docs: List[Document]) -> List[Tuple[Document, float, int]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œå…³é”®è¯æ£€ç´¢æ’åº
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            docs: å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            [(Document, keyword_score, match_count), ...] æŒ‰åˆ†æ•°é™åº
        """
        # æ„å»ºç´¢å¼•
        self._build_keyword_index(docs)
        
        # åˆ†è¯æŸ¥è¯¢
        query_tokens = list(set(self._tokenize(query)))  # å»é‡
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„åŒ¹é…åˆ†æ•°
        doc_scores: Dict[str, float] = defaultdict(float)
        doc_match_count: Dict[str, int] = defaultdict(int)
        
        for token in query_tokens:
            if token in self._keyword_index:
                for doc_id, score in self._keyword_index[token]:
                    doc_scores[doc_id] += score
                    doc_match_count[doc_id] += 1
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°å¹¶æ’åº
        results = []
        for doc_id, base_score in doc_scores.items():
            if doc_id in self._doc_cache:
                # åŒ¹é…è¯æ•°å¥–åŠ±
                match_bonus = 1 + 0.3 * (doc_match_count[doc_id] - 1)
                final_score = base_score * match_bonus
                results.append((self._doc_cache[doc_id], final_score, doc_match_count[doc_id]))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results
    
    def _hybrid_rerank(
        self, 
        query: str, 
        docs: List[Document],
        keyword_boost: float = 0.5,
        keyword_min_score: float = 0.1
    ) -> List[Document]:
        """
        æ··åˆé‡æ’åºï¼šå…³é”®è¯ä¼˜å…ˆ + è¯­ä¹‰æ’åº
        
        ç­–ç•¥ï¼š
        1. å¯¹æ£€ç´¢ç»“æœè¿›è¡Œå…³é”®è¯åŒ¹é…æ‰“åˆ†
        2. å…³é”®è¯åŒ¹é…çš„æ–‡æ¡£è·å¾—å›ºå®šåŠ åˆ†ï¼ˆåªåŠ ä¸€æ¬¡ï¼Œä¸ç´¯åŠ ï¼‰
        3. æ²¡æœ‰å…³é”®è¯åŒ¹é…çš„æ–‡æ¡£ä¿æŒåŸè¯­ä¹‰æ’åº
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            docs: è¯­ä¹‰æ£€ç´¢è¿”å›çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åºï¼‰
            keyword_boost: å…³é”®è¯åŒ¹é…çš„å›ºå®šåŠ åˆ†å€¼ï¼ˆåªåŠ ä¸€æ¬¡ï¼‰
            keyword_min_score: å…³é”®è¯åŒ¹é…çš„æœ€ä½åˆ†æ•°é˜ˆå€¼ï¼ˆå½’ä¸€åŒ–åï¼‰
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not docs:
            return docs
        
        print(f"\n[Agent] æ‰§è¡Œæ··åˆé‡æ’åºï¼ˆå…³é”®è¯ä¼˜å…ˆï¼‰...")
        print(f"[Agent] å‚æ•°: keyword_boost={keyword_boost}, min_score={keyword_min_score}")
        
        # 1. å…³é”®è¯æ£€ç´¢æ‰“åˆ†
        keyword_results = self._keyword_search(query, docs)
        
        # å½’ä¸€åŒ–å…³é”®è¯åˆ†æ•°åˆ° 0-1
        if keyword_results:
            max_score = max(score for _, score, _ in keyword_results)
            min_score = min(score for _, score, _ in keyword_results)
            score_range = max_score - min_score if max_score > min_score else 1
            
            keyword_scores = {}
            for doc, score, match_count in keyword_results:
                doc_id = id(doc)
                norm_score = (score - min_score) / score_range
                keyword_scores[doc_id] = (norm_score, match_count)
        else:
            keyword_scores = {}
        
        # 2. ç»“åˆè¯­ä¹‰æ’åºå’Œå…³é”®è¯æ‰“åˆ†
        # åŸå§‹è¯­ä¹‰æ’åºçš„ä½ç½®åˆ†æ•°ï¼ˆè¶Šé å‰åˆ†æ•°è¶Šé«˜ï¼‰
        results = []
        keyword_matched_count = 0
        
        for rank, doc in enumerate(docs):
            doc_id = id(doc)
            
            # è¯­ä¹‰æ’åºçš„ä½ç½®åˆ†æ•°ï¼ˆå½’ä¸€åŒ–åˆ° 0-1ï¼‰
            semantic_position_score = 1.0 - (rank / len(docs))
            
            # å…³é”®è¯åŒ¹é…åˆ†æ•°
            if doc_id in keyword_scores:
                kw_score, match_count = keyword_scores[doc_id]
                
                if kw_score >= keyword_min_score:
                    # å…³é”®è¯åŒ¹é…ï¼šå›ºå®šåŠ åˆ†ï¼ˆåªåŠ ä¸€æ¬¡ï¼Œä¸ç®¡åŒ¹é…å‡ ä¸ªå…³é”®è¯ï¼‰
                    final_score = semantic_position_score + keyword_boost
                    source = f"keyword({match_count}è¯)"
                    keyword_matched_count += 1
                    is_boosted = True
                else:
                    final_score = semantic_position_score
                    source = "semantic"
                    is_boosted = False
            else:
                final_score = semantic_position_score
                source = "semantic"
                is_boosted = False
            
            results.append((doc, final_score, source, is_boosted, rank))
        
        # 3. æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # 4. æ˜¾ç¤ºæ‰€æœ‰æ–‡æ¡£çš„é‡æ’åºç»“æœ
        print(f"\n[Agent] æ··åˆé‡æ’åºç»“æœï¼ˆå…³é”®è¯åŒ¹é…: {keyword_matched_count}/{len(docs)} ä¸ªæ–‡æ¡£ï¼‰ï¼š")
        print(f"{'æ’å':>4} | {'åŸæ’å':>6} | {'æ¥æº':^15} | {'åˆ†æ•°':>6} | {'æ ‡é¢˜'}")
        print("-" * 80)
        
        for new_rank, (doc, score, source, is_boosted, old_rank) in enumerate(results, 1):
            title = doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
            icon = "ğŸ”‘" if is_boosted else "ğŸ§ "
            rank_change = old_rank + 1 - new_rank
            
            if rank_change > 0:
                change_str = f"â†‘{rank_change}"
            elif rank_change < 0:
                change_str = f"â†“{-rank_change}"
            else:
                change_str = "="
            
            print(f"{new_rank:>4} | {old_rank+1:>4}{change_str:>2} | {icon} {source:<12} | {score:.3f} | {title}")
        
        # è¿”å›æ’åºåçš„æ–‡æ¡£
        reranked_docs = [doc for doc, _, _, _, _ in results]
        print(f"\n[Agent] æ··åˆé‡æ’åºå®Œæˆ")
        
        return reranked_docs

    def _calculate_semantic_similarity(self, query: str, doc: Document) -> float:
        """
        ä½¿ç”¨ embedding æ¨¡å‹è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        
        æ³¨æ„ï¼šè·¯å¾„åŠ æƒå·²ç§»è‡³ PathBoostedRetrieverï¼Œåœ¨æœç´¢é˜¶æ®µç›´æ¥åº”ç”¨
        æ­¤æ–¹æ³•ç°åœ¨ä»…ç”¨äºæ–‡æ¡£å»é‡æ—¶çš„ç›¸ä¼¼åº¦è®¡ç®—
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            doc: æ–‡æ¡£
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not self.embedding_model:
            return 1.0  # å¦‚æœæ²¡æœ‰ embedding æ¨¡å‹ï¼Œé»˜è®¤å…¨éƒ¨é€šè¿‡
        
        try:
            # è·å–æŸ¥è¯¢å’Œæ–‡æ¡£çš„ embedding
            query_embedding = self.embedding_model.embed_query(query)

            doc_text = doc.page_content[:]
            doc_embedding = self.embedding_model.embed_query(doc_text)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_vec = np.array(query_embedding)
            doc_vec = np.array(doc_embedding)
            
            similarity = np.dot(query_vec, doc_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec))
            return float(similarity)
            
        except Exception as e:
            print(f"[Agent] è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 1.0  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡
    
    def _calculate_doc_to_doc_similarity(self, doc1: Document, doc2: Document) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        Args:
            doc1: æ–‡æ¡£1
            doc2: æ–‡æ¡£2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not self.embedding_model:
            return 0.0
        
        try:
            # è·å–ä¸¤ä¸ªæ–‡æ¡£çš„ embedding
            doc1_text = doc1.page_content[:1000]  # é™åˆ¶é•¿åº¦ä»¥åŠ å¿«è®¡ç®—
            doc2_text = doc2.page_content[:1000]
            
            doc1_embedding = self.embedding_model.embed_query(doc1_text)
            doc2_embedding = self.embedding_model.embed_query(doc2_text)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            vec1 = np.array(doc1_embedding)
            vec2 = np.array(doc2_embedding)
            
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            
            return float(similarity)
        except Exception as e:
            print(f"[Agent] è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _deduplicate_and_refill_documents(
        self, 
        query: str, 
        initial_docs: List[Document], 
        target_count: int = 30,
        similarity_threshold: float = 0.80,
        max_attempts: int = 3
    ) -> List[Document]:
        """
        å»é‡ç›¸ä¼¼æ–‡æ¡£å¹¶åŠ¨æ€è¡¥å……ï¼Œç¡®ä¿è¿”å›è¶³å¤Ÿæ•°é‡çš„ç‹¬ç‰¹æ–‡æ¡£
        
        ç­–ç•¥ï¼š
        1. å¯¹åˆå§‹æ–‡æ¡£æŒ‰ç›¸ä¼¼åº¦å»é‡
        2. å¦‚æœå»é‡åä¸è¶³ç›®æ ‡æ•°é‡ï¼Œå¢åŠ æ£€ç´¢æ•°é‡é‡æ–°æ£€ç´¢
        3. å¯¹æ–°æ£€ç´¢ç»“æœå»é‡ï¼ˆåŒ…æ‹¬ä¸å·²ä¿ç•™æ–‡æ¡£æ¯”è¾ƒï¼‰
        4. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡æˆ–è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            initial_docs: åˆå§‹æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            target_count: ç›®æ ‡æ–‡æ¡£æ•°é‡
            similarity_threshold: æ–‡æ¡£é—´ç›¸ä¼¼åº¦é˜ˆå€¼
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°
            
        Returns:
            å»é‡å¹¶è¡¥å……åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.embedding_model:
            print("[Agent] âš ï¸  æœªæä¾› embedding æ¨¡å‹ï¼Œè·³è¿‡æ–‡æ¡£å»é‡")
            return initial_docs[:target_count]
        
        if not initial_docs:
            return initial_docs
        
        print(f"[Agent] æ­£åœ¨è¿›è¡Œæ–‡æ¡£å»é‡ä¸åŠ¨æ€è¡¥å……ï¼ˆç›®æ ‡: {target_count} ä¸ªç‹¬ç‰¹æ–‡æ¡£ï¼Œé˜ˆå€¼: {similarity_threshold}ï¼‰...")
        
        unique_docs = []
        all_checked_docs = set()  # ä½¿ç”¨setå­˜å‚¨å·²æ£€æŸ¥æ–‡æ¡£çš„IDï¼Œé¿å…é‡å¤æ£€æŸ¥
        skipped_count = 0
        
        # ç¬¬ä¸€è½®ï¼šå¤„ç†åˆå§‹æ–‡æ¡£
        print(f"\n[Agent] ç¬¬ 1 è½®ï¼šå¤„ç†åˆå§‹ {len(initial_docs)} ä¸ªæ–‡æ¡£...")
        for i, current_doc in enumerate(initial_docs):
            # ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†
            doc_id = f"{current_doc.metadata.get('full_path', 'unknown')}::{current_doc.page_content[:100]}"
            
            if doc_id in all_checked_docs:
                continue
            all_checked_docs.add(doc_id)
            
            is_duplicate = False
            current_title = current_doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
            
            # ä¸å·²ä¿ç•™çš„æ–‡æ¡£æ¯”è¾ƒ
            for kept_doc in unique_docs:
                doc_similarity = self._calculate_doc_to_doc_similarity(current_doc, kept_doc)
                
                if doc_similarity >= similarity_threshold:
                    # å‘ç°é‡å¤æ–‡æ¡£
                    kept_title = kept_doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
                    if i < 10:  # åªæ˜¾ç¤ºå‰10ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                        print(f"  âœ— è·³è¿‡: {current_title}... (ç›¸ä¼¼åº¦={doc_similarity:.3f}, é‡å¤)")
                    is_duplicate = True
                    skipped_count += 1
                    break
            
            if not is_duplicate:
                unique_docs.append(current_doc)
                if i < 10 or len(unique_docs) <= 5:
                    print(f"  âœ“ ä¿ç•™: {current_title}...")
        
        print(f"[Agent] ç¬¬ 1 è½®å®Œæˆ: {len(initial_docs)} ä¸ªæ–‡æ¡£ â†’ {len(unique_docs)} ä¸ªç‹¬ç‰¹æ–‡æ¡£")
        
        # å¦‚æœå·²ç»è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
        if len(unique_docs) >= target_count:
            print(f"[Agent] âœ“ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ ({target_count})ï¼Œæ— éœ€è¡¥å……")
            return unique_docs[:target_count]
        
        # éœ€è¦è¡¥å……çš„è½®æ¬¡
        attempt = 1
        retrieve_multiplier = 2  # æ¯æ¬¡å¢åŠ æ£€ç´¢æ•°é‡çš„å€æ•°
        
        while len(unique_docs) < target_count and attempt < max_attempts:
            attempt += 1
            needed = target_count - len(unique_docs)
            
            # è®¡ç®—æ–°çš„æ£€ç´¢æ•°é‡ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰
            new_retrieve_count = len(initial_docs) * (retrieve_multiplier ** attempt)
            new_retrieve_count = min(new_retrieve_count, 200)  # æé«˜ä¸Šé™
            
            print(f"\n[Agent] ç¬¬ {attempt} è½®ï¼šè¿˜éœ€ {needed} ä¸ªæ–‡æ¡£ï¼Œæ­£åœ¨æ£€ç´¢ {new_retrieve_count} ä¸ªå€™é€‰...")
            
            try:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç›´æ¥ä» ParentDocumentRetriever çš„ vectorstore æ£€ç´¢æ›´å¤šå­æ–‡æ¡£
                # ä½¿ç”¨ getattr é¿å…ç±»å‹æ£€æŸ¥é”™è¯¯
                if not hasattr(self.retriever, 'vectorstore') or not hasattr(self.retriever, 'docstore'):
                    print(f"[Agent] âš ï¸  æ£€ç´¢å™¨ä¸æ”¯æŒåŠ¨æ€æ•°é‡æ£€ç´¢ï¼Œåœæ­¢è¡¥å……")
                    break
                
                vectorstore = getattr(self.retriever, 'vectorstore')
                docstore = getattr(self.retriever, 'docstore')
                
                # ä»å‘é‡æ•°æ®åº“æ£€ç´¢æ›´å¤šå­æ–‡æ¡£
                child_docs = vectorstore.similarity_search(query, k=new_retrieve_count)
                
                # ä»å­æ–‡æ¡£ ID è·å–çˆ¶æ–‡æ¡£
                more_docs = []
                for child_doc in child_docs:
                    # ParentDocumentRetriever åœ¨å­æ–‡æ¡£çš„ metadata ä¸­å­˜å‚¨çˆ¶æ–‡æ¡£ ID
                    parent_doc_id = child_doc.metadata.get("doc_id")
                    if parent_doc_id and parent_doc_id in docstore.store:
                        parent_doc = docstore.store[parent_doc_id]
                        more_docs.append(parent_doc)
                
                print(f"[Agent] ä» {len(child_docs)} ä¸ªå­æ–‡æ¡£æ£€ç´¢åˆ° {len(more_docs)} ä¸ªçˆ¶æ–‡æ¡£")
                
                if not more_docs or len(more_docs) <= len(all_checked_docs):
                    print(f"[Agent] âš ï¸  æ²¡æœ‰è·å–åˆ°æ–°æ–‡æ¡£ï¼Œåœæ­¢è¡¥å……")
                    break
                
                # è®¡ç®—æ–°æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦å¹¶æ’åº
                new_candidates = []
                for doc in more_docs:
                    doc_id = f"{doc.metadata.get('full_path', 'unknown')}::{doc.page_content[:100]}"
                    
                    # è·³è¿‡å·²æ£€æŸ¥è¿‡çš„æ–‡æ¡£
                    if doc_id in all_checked_docs:
                        continue
                    
                    sim = self._calculate_semantic_similarity(query, doc)
                    new_candidates.append((doc, sim, doc_id))
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                new_candidates.sort(key=lambda x: x[1], reverse=True)
                
                print(f"[Agent] è·å–åˆ° {len(new_candidates)} ä¸ªæ–°å€™é€‰æ–‡æ¡£")
                
                # å¤„ç†æ–°å€™é€‰æ–‡æ¡£
                added_in_round = 0
                for doc, sim, doc_id in new_candidates:
                    if len(unique_docs) >= target_count:
                        break
                    
                    all_checked_docs.add(doc_id)
                    
                    is_duplicate = False
                    current_title = doc.metadata.get('source_title', 'æœªçŸ¥')[:40]
                    
                    # ä¸å·²ä¿ç•™çš„æ–‡æ¡£æ¯”è¾ƒ
                    for kept_doc in unique_docs:
                        doc_similarity = self._calculate_doc_to_doc_similarity(doc, kept_doc)
                        
                        if doc_similarity >= similarity_threshold:
                            if added_in_round < 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ª
                                print(f"  âœ— è·³è¿‡: {current_title}... (ç›¸ä¼¼åº¦={doc_similarity:.3f})")
                            is_duplicate = True
                            skipped_count += 1
                            break
                    
                    if not is_duplicate:
                        unique_docs.append(doc)
                        added_in_round += 1
                        if added_in_round <= 5:
                            print(f"  âœ“ æ–°å¢: {current_title}... (ä¸æŸ¥è¯¢ç›¸ä¼¼åº¦={sim:.3f})")
                
                print(f"[Agent] ç¬¬ {attempt} è½®å®Œæˆ: æ–°å¢ {added_in_round} ä¸ªç‹¬ç‰¹æ–‡æ¡£ï¼Œå½“å‰å…± {len(unique_docs)} ä¸ª")
                
                if added_in_round == 0:
                    print(f"[Agent] âš ï¸  æœ¬è½®æœªæ‰¾åˆ°æ–°çš„ç‹¬ç‰¹æ–‡æ¡£ï¼Œåœæ­¢è¡¥å……")
                    break
                    
            except Exception as e:
                print(f"[Agent] âš ï¸  ç¬¬ {attempt} è½®æ£€ç´¢æ—¶å‡ºé”™: {e}")
                break
        
        final_count = len(unique_docs)
        total_checked = len(all_checked_docs)
        
        print(f"\n[Agent] âœ“ å»é‡ä¸è¡¥å……å®Œæˆ:")
        print(f"    - æ£€æŸ¥äº† {total_checked} ä¸ªæ–‡æ¡£")
        print(f"    - ä¿ç•™äº† {final_count} ä¸ªç‹¬ç‰¹æ–‡æ¡£")
        print(f"    - ç§»é™¤äº† {skipped_count} ä¸ªé‡å¤æ–‡æ¡£")
        print(f"    - è¾¾æˆç‡: {final_count}/{target_count} ({final_count/target_count*100:.1f}%)")
        
        if final_count < target_count:
            print(f"[Agent] âš ï¸  æœªèƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œå¯èƒ½éœ€è¦ï¼š")
            print(f"    1. é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ (å½“å‰: {similarity_threshold})")
            print(f"    2. å¢åŠ åˆå§‹æ£€ç´¢æ•°é‡ (å½“å‰: {len(initial_docs)})")
        
        return unique_docs
    
    def _filter_docs_by_similarity(self, query: str, docs: List[Document], threshold: float = 0.5, mode: str = "rank") -> List[Document]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤æˆ–æ’åºæ–‡æ¡£
        
        âš ï¸ è­¦å‘Šï¼šæ­¤æ–¹æ³•ä¼šé‡æ–°è®¡ç®—åŸå§‹è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œä¼šè¦†ç›– PathBoostedRetriever çš„è·¯å¾„åŠ æƒï¼
        å¦‚æœä½¿ç”¨ PathBoostedRetrieverï¼Œå»ºè®®ç¦ç”¨æ­¤åŠŸèƒ½ (ENABLE_SEMANTIC_FILTER = False)
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²ç»è¿‡è·¯å¾„åŠ æƒæ’åºï¼‰
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)ï¼Œä»…åœ¨ mode="threshold" æ—¶ä½¿ç”¨
            mode: "rank" (æŒ‰ç›¸ä¼¼åº¦æ’åº) æˆ– "threshold" (è¿‡æ»¤ä½äºé˜ˆå€¼çš„æ–‡æ¡£)
            
        Returns:
            è¿‡æ»¤/æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.embedding_model:
            print("[Agent] âš ï¸  æœªæä¾› embedding æ¨¡å‹ï¼Œè·³è¿‡è¯­ä¹‰è¿‡æ»¤")
            return docs
        
        if not docs:
            return docs
        
        if mode == "rank":
            print(f"[Agent] æ­£åœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶æ’åº...")
        else:
            print(f"[Agent] æ­£åœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆé˜ˆå€¼: {threshold}ï¼‰...")
        
        # è®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
        doc_scores = []
        
        for i, doc in enumerate(docs):
            similarity = self._calculate_semantic_similarity(query, doc)
            doc_scores.append({
                'doc': doc,
                'similarity': similarity,
                'index': i
            })
        
        if mode == "rank":
            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            doc_scores.sort(key=lambda x: x['similarity'], reverse=True)
            
            # æ˜¾ç¤ºæ’åºç»“æœï¼ˆå‰15ä¸ªï¼Œå› ä¸ºåé¢ä¼šå»é‡ï¼‰
            print(f"\n[Agent] æ–‡æ¡£ç›¸ä¼¼åº¦æ’åï¼ˆå‰ {min(15, len(doc_scores))} ä¸ªï¼‰ï¼š")
            for rank, item in enumerate(doc_scores[:15], 1):
                title = item['doc'].metadata.get('source_title', 'æœªçŸ¥')
                full_path = item['doc'].metadata.get('full_path', 'æœªçŸ¥')
                category = full_path.split('/')[0] if '/' in full_path else 'æœªçŸ¥'
                
                print(f"  {rank}. [{category}] {title[:]}... ç›¸ä¼¼åº¦={item['similarity']:.3f}")
            
            # è¿”å›æ’åºåçš„æ–‡æ¡£
            filtered_docs = [item['doc'] for item in doc_scores]
            print(f"\n[Agent] è¯­ä¹‰æ’åºå®Œæˆ: {len(docs)} ä¸ªæ–‡æ¡£ â†’ {len(filtered_docs)} ä¸ªæ–‡æ¡£")
            
        else:  # mode == "threshold"
            # æŒ‰é˜ˆå€¼è¿‡æ»¤
            filtered_docs = []
            for item in doc_scores:
                title = item['doc'].metadata.get('source_title', 'æœªçŸ¥')
                print(f"  æ–‡æ¡£ {item['index']+1}: {title[:]}... ç›¸ä¼¼åº¦={item['similarity']:.3f}", end="")
                
                if item['similarity'] >= threshold:
                    filtered_docs.append(item['doc'])
                    print(" âœ“ ä¿ç•™")
                else:
                    print(" âœ— è¿‡æ»¤")
            
            print(f"[Agent] è¯­ä¹‰è¿‡æ»¤: {len(docs)} ä¸ªæ–‡æ¡£ â†’ {len(filtered_docs)} ä¸ªæ–‡æ¡£")
        
        return filtered_docs
    
    def _format_documents(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not docs:
            return "æœªæ£€ç´¢åˆ°ç›¸å…³è§„åˆ™æ–‡æ¡£ã€‚"
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            full_path = doc.metadata.get('full_path', 'æœªçŸ¥æ¥æº')
            source_title = doc.metadata.get('source_title', 'æœªçŸ¥æ ‡é¢˜')
            content = doc.page_content
            
            formatted.append(f"""
--- æ–‡æ¡£ {i} ---
**æ¥æºè·¯å¾„**: {full_path}
**æ ‡é¢˜**: {source_title}

{content}
---
""")
        
        return "\n".join(formatted)

    def invoke(self, input_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›å“åº”ã€‚
        æ”¯æŒåŠ¨æ€è°ƒæ•´æ–‡æ¡£æ•°é‡ä»¥åº”å¯¹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚
        
        Args:
            input_dict: åŒ…å« 'input' é”®çš„å­—å…¸
            
        Returns:
            åŒ…å« 'output' é”®çš„å­—å…¸
        """
        user_input = input_dict.get("input", "æœªçŸ¥è¾“å…¥")
        
        try:
            print(f"\n[Agent] ç”¨æˆ·è¾“å…¥: {user_input}")
            
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆå°½å¯èƒ½å¤šï¼‰
            initial_retrieve_count = config.PARENT_RETRIEVER_TOP_K
            print(f"[Agent] æ­£åœ¨æ£€ç´¢æ–‡æ¡£ï¼ˆåˆå§‹æ£€ç´¢: {initial_retrieve_count} ä¸ªï¼‰...")
            retrieved_docs = self.retriever.invoke(user_input)
            print(f"[Agent] æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
            
            # 2. æ··åˆé‡æ’åºï¼šå…³é”®è¯ä¼˜å…ˆ + è¯­ä¹‰æ’åºï¼ˆå¯é…ç½®ï¼‰
            # è¿™ä¼šå¯¹è¯­ä¹‰æ£€ç´¢çš„ç»“æœè¿›è¡ŒäºŒæ¬¡æ’åºï¼Œè®©ç²¾ç¡®åŒ¹é…å…³é”®è¯çš„æ–‡æ¡£æ’åœ¨å‰é¢
            if getattr(config, 'ENABLE_HYBRID_RETRIEVAL', False):
                keyword_boost = getattr(config, 'KEYWORD_MATCH_BOOST', 0.5)
                keyword_min_score = getattr(config, 'KEYWORD_MIN_SCORE_THRESHOLD', 0.1)
                
                retrieved_docs = self._hybrid_rerank(
                    user_input,
                    retrieved_docs,
                    keyword_boost=keyword_boost,
                    keyword_min_score=keyword_min_score
                )
            
            # 3. è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº/è¿‡æ»¤ï¼ˆå¯é…ç½®ï¼‰
            # âš ï¸ æ³¨æ„ï¼šPathBoostedRetriever å·²ç»åœ¨æ£€ç´¢é˜¶æ®µå®Œæˆäº†è·¯å¾„åŠ æƒæ’åº
            # å¦‚æœå¯ç”¨æ­¤é€‰é¡¹ï¼Œä¼šé‡æ–°è®¡ç®—åŸå§‹ç›¸ä¼¼åº¦ï¼Œè¦†ç›–æ‰è·¯å¾„åŠ æƒçš„æ•ˆæœï¼
            # å»ºè®®ï¼šå¦‚æœä½¿ç”¨ PathBoostedRetrieverï¼Œåº”ç¦ç”¨æ­¤é€‰é¡¹ (ENABLE_SEMANTIC_FILTER = False)
            elif hasattr(config, 'ENABLE_SEMANTIC_FILTER') and config.ENABLE_SEMANTIC_FILTER:
                filter_mode = getattr(config, 'SEMANTIC_FILTER_MODE', 'rank')
                similarity_threshold = getattr(config, 'SEMANTIC_SIMILARITY_THRESHOLD', 0.4)
                
                print(f"[Agent] âš ï¸  è­¦å‘Šï¼šå¯ç”¨è¯­ä¹‰é‡æ’åºä¼šè¦†ç›– PathBoostedRetriever çš„è·¯å¾„åŠ æƒï¼")
                retrieved_docs = self._filter_docs_by_similarity(
                    user_input, 
                    retrieved_docs, 
                    similarity_threshold,
                    mode=filter_mode
                )
            else:
                print(f"[Agent] é‡æ’åºå·²ç¦ç”¨ï¼ˆä¿ç•™æ£€ç´¢å™¨çš„åŸå§‹æ’åºï¼‰")
            
            # 4. æ–‡æ¡£å»é‡å¹¶è¡¥å……ï¼šç§»é™¤å†…å®¹ç›¸ä¼¼çš„é‡å¤æ–‡æ¡£ï¼Œå¹¶åŠ¨æ€è¡¥å……ï¼ˆå¯é…ç½®ï¼‰
            if hasattr(config, 'ENABLE_DOCUMENT_DEDUPLICATION') and config.ENABLE_DOCUMENT_DEDUPLICATION:
                dedup_threshold = getattr(config, 'DOCUMENT_SIMILARITY_THRESHOLD', 0.80)
                target_doc_count = getattr(config, 'PARENT_RETRIEVER_TOP_K', 30)
                max_attempts = getattr(config, 'MAX_DEDUP_ATTEMPTS', 3)
                
                retrieved_docs = self._deduplicate_and_refill_documents(
                    user_input,
                    retrieved_docs,
                    target_count=target_doc_count,
                    similarity_threshold=dedup_threshold,
                    max_attempts=max_attempts
                )
            else:
                print(f"[Agent] æ–‡æ¡£å»é‡å·²ç¦ç”¨ï¼Œè·³è¿‡å»é‡")
            
            # 4. é™åˆ¶æ–‡æ¡£æ•°é‡åˆ°å½“å‰è®¾å®šå€¼ï¼ˆå–å‰Nä¸ªæœ€ç›¸å…³çš„ï¼‰
            final_doc_count = min(self.current_doc_count, len(retrieved_docs))
            retrieved_docs = retrieved_docs[:final_doc_count]
            print(f"[Agent] æœ€ç»ˆä½¿ç”¨ {len(retrieved_docs)} ä¸ªæ–‡æ¡£ï¼ˆæœ€ç›¸å…³çš„å‰ {final_doc_count} ä¸ªï¼‰")
            
            # 5. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self._format_documents(retrieved_docs)
            
            # 6. è°ƒç”¨ Geminiï¼ˆä¸ä¼ é€’å†å²ï¼Œåªä¼ é€’å½“å‰é—®é¢˜å’Œè§„åˆ™æ–‡æ¡£ï¼‰
            print("[Agent] æ­£åœ¨ç”Ÿæˆå›ç­”...")
            response = self.chain.invoke({
                "context": context,
                "input": user_input
            })
            
            # 7. æ‹¼æ¥æ–‡æ¡£æ¥æºè·¯å¾„åˆ°å›ç­”æœ«å°¾
            doc_sources = []
            for doc in retrieved_docs:
                full_path = doc.metadata.get('full_path', 'æœªçŸ¥æ¥æº')
                if full_path not in doc_sources:  # å»é‡
                    doc_sources.append(full_path)
            
            if doc_sources:
                sources_text = "\n\n" + "="*50 + "\n"
                sources_text += "ğŸ“š **å‚è€ƒçš„è§„åˆ™æ–‡æ¡£æ¥æº**ï¼š\n\n"
                for i, source in enumerate(doc_sources, 1):
                    sources_text += f"{i}. {source}\n"
                response_with_sources = response + sources_text
            else:
                response_with_sources = response
            
            # æˆåŠŸï¼å°è¯•å¢åŠ æ–‡æ¡£æ•°é‡ï¼ˆæ¸è¿›å¼å¢åŠ ï¼‰
            if self.current_doc_count < self.max_doc_count:
                self.current_doc_count = min(self.current_doc_count + 1, self.max_doc_count)
                print(f"[Agent] âœ“ å“åº”æˆåŠŸï¼Œä¸‹æ¬¡å°†å°è¯•ä½¿ç”¨ {self.current_doc_count} ä¸ªæ–‡æ¡£")
            
            # 8. ä¿å­˜å¯¹è¯å†å²ï¼ˆä»…ç”¨äºæœ¬åœ°è®°å½•ï¼Œä¸ä¼ é€’ç»™æ¨¡å‹ï¼‰
            self.chat_history.append((user_input, response_with_sources))
            if len(self.chat_history) > self.history_k:
                self.chat_history = self.chat_history[-self.history_k:]
            
            print("[Agent] å›ç­”ç”Ÿæˆå®Œæˆ")
            
            return {"output": response_with_sources}
            
        except Exception as e:
            error_str = str(e)
            
            # æ£€æµ‹æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™é”™è¯¯
            if "context length" in error_str.lower() or "max" in error_str.lower() and "token" in error_str.lower():
                print(f"[Agent] âš ï¸  ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™")
                
                # å‡å°‘æ–‡æ¡£æ•°é‡å¹¶é‡è¯•
                if self.current_doc_count > self.min_doc_count:
                    self.current_doc_count = max(self.current_doc_count - 4, self.min_doc_count)
                    print(f"[Agent] ğŸ“‰ å‡å°‘æ–‡æ¡£æ•°é‡åˆ° {self.current_doc_count}ï¼Œæ­£åœ¨é‡è¯•...")
                    
                    # é€’å½’é‡è¯•
                    return self.invoke(input_dict)
                else:
                    error_msg = f"æŠ±æ­‰ï¼Œå³ä½¿ä½¿ç”¨æœ€å°‘æ–‡æ¡£æ•°é‡ï¼ˆ{self.min_doc_count}ï¼‰ä»ç„¶è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶ã€‚è¯·å°è¯•æ›´ç®€çŸ­çš„é—®é¢˜ã€‚"
                    print(f"[Agent] âŒ {error_msg}")
                    self.chat_history.append((user_input, error_msg))
                    return {"output": error_msg}
            
            # å…¶ä»–é”™è¯¯
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {error_str}"
            print(f"[Agent] é”™è¯¯: {e}")
            
            # ä¿å­˜é”™è¯¯åˆ°å†å²
            self.chat_history.append((user_input, error_msg))
            
            return {"output": error_msg}


def create_gemini_agent_executor(
    llm: ChatGoogleGenerativeAI,
    retriever: BaseRetriever,
    embedding_model=None
) -> GeminiAgentExecutor:
    """
    åˆ›å»º Agent æ‰§è¡Œå™¨ã€‚
    
    Args:
        llm: Gemini LLM å®ä¾‹
        retriever: çˆ¶æ–‡æ¡£æ£€ç´¢å™¨å®ä¾‹
        embedding_model: Embedding æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰
        
    Returns:
        GeminiAgentExecutor å®ä¾‹
    """
    print("[Agent] æ­£åœ¨åˆ›å»º Agent æ‰§è¡Œå™¨...")
    if embedding_model:
        print("[Agent] âœ“ å·²å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ï¼ˆåŸºäº embedding æ¨¡å‹ï¼‰")
    return GeminiAgentExecutor(llm=llm, retriever=retriever, embedding_model=embedding_model)
